from moviepy.video.io.VideoFileClip import VideoFileClip
from faster_whisper import WhisperModel
import os
import google.generativeai as genai
import whisper
import time
from tqdm import tqdm

# é…ç½®Gemini API
def configure_gemini_api():
    genai.configure(api_key="AIzaSyDXTjBu00nQP0V5B-CTwVzZylDeXQW3VqY")  # è¯·æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    return genai.GenerativeModel('gemini-2.0-flash')

# ä½¿ç”¨Geminiç¿»è¯‘æ–‡æœ¬
def translate_with_gemini(model, text, source_lang="ja", target_lang="zh"):
    prompt = (
        f"è¯·å°†ä»¥ä¸‹{source_lang}æ–‡æœ¬ç¿»è¯‘ä¸º{target_lang}ï¼Œè¦æ±‚è¡¨è¾¾è¦ç¨å¾®è‡ªç„¶ã€æ¥åœ°æ°”ã€å£è¯­åŒ–ï¼Œä½†ä¸èƒ½è¿‡äºåç¦»åŸæœ¬è¯­ä¹‰ï¼Œ"
        f"å¹¶ä¸”ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€æ ¼å¼è¯´æ˜æˆ–å…¶ä»–æ— å…³å†…å®¹ï¼Œä»…è¾“å‡ºç¿»è¯‘ç»“æœï¼š\n{text}"
    )
    response = model.generate_content(prompt)
    return response.text.strip()

# æå–éŸ³é¢‘
def extract_audio(video_path, audio_path="temp_audio.wav"):
    video = VideoFileClip(video_path)
    audio = video.audio
    audio.write_audiofile(audio_path)
    video.close()
    audio.close()

# ä½¿ç”¨ faster-whisper è¿›è¡Œè½¬å½•

def transcribe_with_fast_whisper(audio_path):
    model = WhisperModel("large-v2", device="cuda", compute_type="float32")
    print("ğŸ§  Fast-Whisper æ­£åœ¨è¯­éŸ³è¯†åˆ«ä¸­...")

    segments, _ = model.transcribe(audio_path, beam_size=5, language="ja")

    results = []
    for segment in segments:
        start = segment.start
        end = segment.end
        text = segment.text.strip()
        results.append((start, end, text))

    print(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆï¼Œå…± {len(results)} æ®µ")
    return results

# ä½¿ç”¨ openai whisper è¿›è¡Œè½¬å½•
def transcribe_with_whisper(audio_path):
    model = whisper.load_model("large-v3", device="cuda")
    result = model.transcribe(audio_path, beam_size=5, language="ja")

    segments = result["segments"]
    results = []
    print("ğŸ§  Whisper æ­£åœ¨è¯­éŸ³è¯†åˆ«ä¸­...")

    for segment in tqdm(segments, desc="ğŸ“ è½¬å½•è¿›åº¦", ncols=70, bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}%"):
        start = segment["start"]
        end = segment["end"]
        text = segment["text"].strip()
        results.append((start, end, text))

    print(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆï¼Œå…± {len(results)} æ®µ")
    return results

# æ ¼å¼åŒ– ASS æ—¶é—´
def format_ass_time(seconds):
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    cs = int((seconds - int(seconds)) * 100)
    return f"{h}:{m:02d}:{s:02d}.{cs:02d}"

# ç”Ÿæˆ ASS å­—å¹•æ–‡ä»¶
def create_new_ass(segments, translations, output_path):
    header = """[Script Info]
Title: Generated by Whisper with Gemini Translation
ScriptType: v4.00+
Collisions: Normal
PlayResX: 1920
PlayResY: 1080
Timer: 100.0000

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,å¾®è½¯é›…é»‘,60,&H00FFFFFF,&H000000FF,&H00000000,&H64000000,-1,0,0,0,100,100,0,0,1,2.0,2.0,2,10,10,10,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
    dialogue_lines = []
    for (start, end, text), translated_text in zip(segments, translations):
        start_ts = format_ass_time(start)
        end_ts = format_ass_time(end)
        original_line = f"Dialogue: 0,{start_ts},{end_ts},Default,,0,0,0,,{text}"
        translated_line = f"Dialogue: 0,{start_ts},{end_ts},Default,,0,0,0,,{translated_text}"
        dialogue_lines.append(original_line)
        dialogue_lines.append(translated_line)

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(header + "\n".join(dialogue_lines))

# ä¸»æµç¨‹
def main():
    video_path = input("è¯·è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼š").strip().strip('"').strip("'")
    if not os.path.exists(video_path):
        print("âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨")
        return

    audio_path = "temp_audio.wav"
    output_ass_path = os.path.splitext(video_path)[0] + ".ass"

    # æå–éŸ³é¢‘
    extract_audio(video_path, audio_path)

    # è½¬å½•éŸ³é¢‘
    segments = transcribe_with_fast_whisper(audio_path)

    # ç¿»è¯‘æ–‡æœ¬
    print("ğŸŒ å¼€å§‹ç¿»è¯‘æ–‡æœ¬...")
    gemini_model = configure_gemini_api()
    translations = []

    with tqdm(total=len(segments), desc="ç¿»è¯‘è¿›åº¦", ncols=70, bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt}") as pbar:
        for _, _, text in segments:
            translated = translate_with_gemini(gemini_model, text)
            translations.append(translated)
            pbar.update(1)

    # ç”Ÿæˆå­—å¹•æ–‡ä»¶
    print("ğŸ’¾ ç”Ÿæˆå­—å¹•æ–‡ä»¶ä¸­...")
    create_new_ass(segments, translations, output_ass_path)
    print("âœ… å­—å¹•æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š", output_ass_path)

    # æ¸…ç†ä¸´æ—¶éŸ³é¢‘
    if os.path.exists(audio_path):
        os.remove(audio_path)

    print("ğŸ§¹ æ¸…ç†å®Œæˆï¼å…¨éƒ¨ä»»åŠ¡å·²å®Œæˆã€‚")

if __name__ == "__main__":
    main()
